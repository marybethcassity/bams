{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from bams.data import KeypointsDataset\n",
    "from bams.models import BAMS\n",
    "from bams import compute_representations\n",
    "from custom_dataset_w_labels import load_data, load_annotations\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input the path to the model folder and the data folder \n",
    "\n",
    "the data folder should be organized with subfolders named with the species/label. \n",
    "each subfolder contains dlc csvs. each csv should contain the same number of datapoints (frames) \n",
    "'''\n",
    "\n",
    "### input path to data folder here ###\n",
    "data_folder = r\"X:\\Behavior\\DeepLabCut\\Body_Models_2D\\Movement_Curated\\completedata\\rats_treeshrews\\threshold_0.8\\movement\" \n",
    "\n",
    "### input path to model folder here ###\n",
    "model_folder = r\"X:\\MaryBeth\\BAMS\\completedata\\bams-custom-2024-08-29-15-48-48_0.8\" \n",
    "\n",
    "with os.scandir(model_folder) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_file() and entry.name.startswith('bams-custom') and entry.name.endswith('.pt'):\n",
    "            model_name = entry\n",
    "            print(\"Loading model\", model_name)\n",
    "            print()\n",
    "            \n",
    "model_path = os.path.join(model_folder, model_name)\n",
    "annotations_path = os.path.join(model_folder,\"video_labels.csv\")\n",
    "\n",
    "hoa_bins = 32\n",
    "keypoints = load_data(data_folder, model_folder, create_csv = False) ### set to True if you want to re create the mapping from bams to the original data csvs ### \n",
    "annotations, eval_utils = load_annotations(annotations_path)\n",
    "\n",
    "dataset = KeypointsDataset(\n",
    "        keypoints=keypoints,\n",
    "        cache=False,\n",
    "        hoa_bins=hoa_bins,\n",
    "        annotations=annotations,\n",
    "        eval_utils=eval_utils\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BAMS(\n",
    "        input_size=dataset.input_size,\n",
    "        short_term=dict(num_channels=(64, 64, 64, 64), kernel_size=3),\n",
    "        long_term=dict(num_channels=(64, 64, 64, 64, 64), kernel_size=3, dilation=4),\n",
    "        predictor=dict(\n",
    "            hidden_layers=(-1, 256, 512, 512, dataset.target_size * hoa_bins)\n",
    "        ),\n",
    "    ).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "embeddings = compute_representations(model, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PCAs of sequence level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "each embedding (short, long, all) should be shape (n samples (total csvs), n frames per sample, n bams features)\n",
    "'''\n",
    "\n",
    "### retrieve frame level embeddings ###\n",
    "\n",
    "short_term = embeddings['short_term']\n",
    "long_term = embeddings['long_term']\n",
    "all_embeddings = torch.cat([short_term, long_term], dim=2)\n",
    "\n",
    "print(\"short_term: \", np.shape(short_term))\n",
    "print(\"long_term: \", np.shape(long_term))\n",
    "print(\"all_embeddings: \", np.shape(all_embeddings))\n",
    "print(\"\")\n",
    "\n",
    "### compute sequence level embeddings of the bams features by averaging over the frames for each sample ###\n",
    "\n",
    "short_term_seq = torch.mean(short_term, dim=1, keepdim=False)\n",
    "long_term_seq = torch.mean(long_term, dim=1, keepdim=False)\n",
    "all_embeddings_seq = torch.cat([short_term_seq, long_term_seq], dim=1)\n",
    "\n",
    "print(\"short_term: \", np.shape(short_term_seq))\n",
    "print(\"long_term: \", np.shape(long_term_seq))\n",
    "print(\"all_embeddings: \", np.shape(all_embeddings_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fit PCA for each sequence level embedding ###\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(short_term_seq)\n",
    "short_term_pca = pca.transform(short_term_seq)\n",
    "ev_short_term = pca.explained_variance_ratio_\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(long_term_seq)\n",
    "long_term_pca = pca.transform(long_term_seq)\n",
    "ev_long_term = pca.explained_variance_ratio_\n",
    "\n",
    "pca.fit(all_embeddings_seq)\n",
    "all_pca = pca.transform(all_embeddings_seq)\n",
    "ev_all = pca.explained_variance_ratio_\n",
    "\n",
    "### print the explaiened variance for each emebddings ###\n",
    "\n",
    "print(\"ev short term: \", ev_short_term)\n",
    "print(\"ev long term: \", ev_long_term)\n",
    "print(\"ev all: \", ev_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create mapping for PCA legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### load the mapping from bams to the original data csvs ### \n",
    "\n",
    "df = pd.read_csv(annotations_path)\n",
    "\n",
    "num_segments = len(df)\n",
    "video_names = df['video_name'].unique()\n",
    "unique_labels = df['label'].unique()\n",
    "label_mapping = {label: index for index, label in enumerate(unique_labels)}\n",
    "\n",
    "print(label_mapping)\n",
    "\n",
    "video_label_mapping = df.groupby('video_name')['label'].first().to_dict()\n",
    "\n",
    "print(video_label_mapping)\n",
    "\n",
    "consolidated_video_label_mapping = {}\n",
    "\n",
    "for video_name, label in video_label_mapping.items():\n",
    "    ### extract the date from each video name ###\n",
    "    starting_number = video_name.split('_')[0]\n",
    "    \n",
    "    ### add each unique date to the consolidated mapping ###\n",
    "    if starting_number not in consolidated_video_label_mapping:\n",
    "        consolidated_video_label_mapping[starting_number] = label\n",
    "\n",
    "print(consolidated_video_label_mapping)\n",
    "\n",
    "videos_label_1 = [video for video, label in consolidated_video_label_mapping.items() if label == 'treeshrew']\n",
    "videos_label_0 = [video for video, label in consolidated_video_label_mapping.items() if label == 'rat']\n",
    "\n",
    "print(videos_label_1)\n",
    "print(videos_label_0)\n",
    "\n",
    "count_treeshrew = df[df['label'] == 'treeshrew'].shape[0]\n",
    "count_rat = df[df['label'] == 'rat'].shape[0]\n",
    "\n",
    "print(\"num rats: \", count_rat)\n",
    "print(\"num treeshrews: \", count_treeshrew)\n",
    "\n",
    "### make color gradients for each species ### \n",
    "num_rat_videos = len(videos_label_0)\n",
    "num_treeshrew_videos = len(videos_label_1)\n",
    "\n",
    "rat_colormap = plt.cm.summer(np.linspace(0, 1, num_rat_videos))\n",
    "ts_colormap = plt.cm.cool(np.linspace(0, 1, num_treeshrew_videos))\n",
    "\n",
    "video_color_mapping = {video: tuple(ts_colormap[i]) for i, video in enumerate(videos_label_1)}\n",
    "video_color_mapping.update({video: tuple(rat_colormap[i]) for i, video in enumerate(videos_label_0)})\n",
    "\n",
    "print(video_color_mapping)\n",
    "\n",
    "def extract_numbers(video_name):\n",
    "    match = re.match(r'(\\d+)', video_name)\n",
    "    return match.group(1) if match else video_name\n",
    "\n",
    "video_number_color_mapping = {extract_numbers(video): color for video, color in video_color_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D Visualization with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "df['starting_number'] = df['video_name'].apply(lambda x: x.split('_')[0])\n",
    "c = df['starting_number'].map(video_color_mapping) \n",
    "\n",
    "### plot the pcas for the short term, long term, and all embeddings ### \n",
    "ax[0].scatter(short_term_pca[:, 0], short_term_pca[:, 1], c=c, s=10)\n",
    "ax[0].set_title(f'Short term PCA explained variance: {round(sum(ev_short_term)*100)}%')\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].scatter(long_term_pca[:, 0], long_term_pca[:, 1], c=c, s=10)\n",
    "ax[1].set_title(f'Long term PCA explained variance: {round(sum(ev_long_term)*100)}%')\n",
    "ax[2].scatter(all_pca[:, 0], all_pca[:, 1], c=c, s=10)\n",
    "ax[2].set_title(f'All PCA explained variance: {round(sum(ev_all)*100)}%')\n",
    "\n",
    "handles_numbers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in video_number_color_mapping.values()]\n",
    "labels_numbers = [number for number in video_number_color_mapping.keys()]\n",
    "\n",
    "treeshrew_handle = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in ts_colormap]\n",
    "rat_handle = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in rat_colormap]\n",
    "\n",
    "handles_species = treeshrew_handle + rat_handle\n",
    "labels_species = [f'Treeshrew' for i in range(len(ts_colormap))] + [f'Rat' for i in range(len(rat_colormap))]\n",
    "\n",
    "labels_species[0] = 'Treeshrew (1)'\n",
    "labels_species[1] = 'Treeshrew (2)'\n",
    "labels_species[2] = 'Treeshrew (2)'\n",
    "labels_species[3] = 'Treeshrew (2)'\n",
    "labels_species[4] = 'Treeshrew (2)'\n",
    "\n",
    "first_legend = fig.legend(handles_numbers, labels_numbers, loc='upper right', title='Video', bbox_to_anchor=(1, 0.8))\n",
    "second_legend = fig.legend(handles_species, labels_species, loc='upper left', title='Species', bbox_to_anchor=(1, 0.8))\n",
    "\n",
    "plt.gca().add_artist(first_legend)\n",
    "plt.gca().add_artist(second_legend)\n",
    "plt.show()\n",
    "\n",
    "### uncomment if you want to save the 2d pca plot ###\n",
    "# plt.savefig(os.path.join(model_folder,\"2dpca.png\"),bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Pearson correlation matrices for the short and long term sequence level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_term_seq_np = short_term_seq.numpy()\n",
    "long_term_seq_np = long_term_seq.numpy()\n",
    "all_embeddings_seq_np = all_embeddings_seq.numpy()\n",
    "\n",
    "print(\"short_term: \", np.shape(short_term_seq_np))\n",
    "print(\"long_term: \", np.shape(long_term_seq_np))\n",
    "\n",
    "def plot_correlation_heatmap(ax, data, title):\n",
    "    corr_matrix = np.corrcoef(data, rowvar=False) ### setting rowvar=False because the columns contain the features and the rows contain the observations ###\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", ax=ax, vmin=-1, vmax=1)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ticks = np.arange(0, data.shape[1], 2)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(ticks)\n",
    "    ax.set_yticklabels(ticks)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(24, 8))\n",
    "\n",
    "plot_correlation_heatmap(ax[0], short_term_seq_np, \"Short Term Sequence\")\n",
    "plot_correlation_heatmap(ax[1], long_term_seq_np, \"Long Term Sequence\")\n",
    "\n",
    "### uncomment if you want to save the pearson correlation plot ###\n",
    "#plt.savefig(os.path.join(model_folder,\"pc_seq.png\"),bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3D UMAP to explore frame level emebddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reshape frame array ###\n",
    "\n",
    "short_term_reshaped = short_term.view(short_term.size(0) * short_term.size(1), short_term.size(2))\n",
    "long_term_reshaped = long_term.view(long_term.size(0) * long_term.size(1), long_term.size(2))\n",
    "all_embeddings_reshaped = all_embeddings.view(all_embeddings.size(0) * all_embeddings.size(1), all_embeddings.size(2))\n",
    "\n",
    "### print shapes before and after reshaping ###\n",
    "\n",
    "print(\"short_term: \", np.shape(short_term))\n",
    "print(\"long_term: \", np.shape(long_term))\n",
    "print(\"all_embeddings: \", np.shape(all_embeddings))\n",
    "print(\"\")\n",
    "\n",
    "print(\"short_term: \", np.shape(short_term_reshaped))\n",
    "print(\"long_term: \", np.shape(long_term_reshaped))\n",
    "print(\"all_embeddings: \", np.shape(all_embeddings_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create mapping from bams embeddings to the video/ csv space ###\n",
    "\n",
    "### read the order in which the csvs were passed to bams ###\n",
    "df = pd.read_csv(annotations_path)\n",
    "video = df['video_name']\n",
    "\n",
    "### for each csv, repeat its name n frames per sample times ###\n",
    "video = np.repeat(video, short_term.shape[1])\n",
    "print(np.shape(video))\n",
    "\n",
    "### for each csv, number the frames from 0 to n frames per sample ### \n",
    "repeated_array = np.tile(np.arange(short_term.shape[1]), short_term.shape[0])\n",
    "print(np.shape(repeated_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make mapping that contains [video name, frame number] for each sample ###\n",
    "video_frames = np.column_stack((video, repeated_array))\n",
    "\n",
    "print(np.shape(video_frames))\n",
    "\n",
    "### check ###\n",
    "print(video_frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### just checking ###\n",
    "\n",
    "for i in video_frames:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import kneed\n",
    "\n",
    "### how many neighbors will be used for DBSCAN? ###\n",
    "k = 5\n",
    "\n",
    "### randomly sample the input data for time/computation efficiency ### \n",
    "\n",
    "### choose how many samples you want to plot ###\n",
    "sample_size = 1000\n",
    "\n",
    "sample_indices = np.random.choice(len(short_term_reshaped), size=sample_size, replace=False)\n",
    "\n",
    "short_sample = short_term_reshaped[sample_indices]\n",
    "video_frames_sample = video_frames[sample_indices]\n",
    "\n",
    "### for automating eps selection, you can use the knee method ### \n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(short_sample)\n",
    "\n",
    "distances, indeces = nbrs.kneighbors(short_sample)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = kneed.KneeLocator(i, distances, curve='convex', direction='increasing', online = False)\n",
    "x = distances[knee.knee]\n",
    "\n",
    "print(\"eps :\", x)\n",
    "\n",
    "plt.axhline(y=x, color='r', linestyle='-')\n",
    "\n",
    "plt.plot(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "umap_model = umap.UMAP(n_components=3, random_state=42) \n",
    "short_umap = umap_model.fit_transform(short_sample)\n",
    "\n",
    "### set eps to a computed or  experimentally determined value ###\n",
    "dbscan_model = DBSCAN(eps=0.15, min_samples=k) \n",
    "dbscan_labels = dbscan_model.fit_predict(short_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check shapes to make sure everything makes sense ###\n",
    "\n",
    "print(np.shape(short_sample))\n",
    "print(np.shape(video_frames_sample))\n",
    "print(np.shape(dbscan_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "def index_to_time(index, frame_rate=30):\n",
    "\n",
    "    total_seconds = index // frame_rate\n",
    "    minutes = total_seconds // 60\n",
    "    seconds = total_seconds % 60\n",
    "    \n",
    "    return f\"{minutes}:{seconds:02d}\"\n",
    "\n",
    "unique_labels = np.unique(dbscan_labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "traces = []\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    ### get points with the current label ###\n",
    "    cluster_points = short_umap[dbscan_labels == label]\n",
    "    video_frame_text = video_frames_sample[dbscan_labels == label]\n",
    "\n",
    "    # print(np.shape(cluster_points))\n",
    "    # print(np.shape(video_frame_text))\n",
    "    \n",
    "    ### add trace for the current label ### \n",
    "    traces.append(go.Scatter3d(\n",
    "        x=cluster_points[:, 0],\n",
    "        y=cluster_points[:, 1],\n",
    "        z=cluster_points[:, 2],\n",
    "        mode='markers',\n",
    "        name=f'Cluster {label}' if label != -1 else 'Noise',\n",
    "        marker=dict(\n",
    "            size=3,\n",
    "            color=f'rgb({color[0]*255},{color[1]*255},{color[2]*255})',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        text=[f'{vf[0]} {vf[1]} {index_to_time(vf[1])}' for vf in video_frame_text],\n",
    "    ))\n",
    "\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='UMAP 1'),\n",
    "        yaxis=dict(title='UMAP 2'),\n",
    "        zaxis=dict(title='UMAP 3')\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0),  \n",
    "    legend=dict(title='DBSCAN Clusters')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "### uncomment if you want to save the umap ###\n",
    "# fig.write_html(os.path.join(model_folder,\"short_term_dbscan_5_0.15.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the samples from each cluster as gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# ''''\n",
    "# to save each data point as a gif to its cluster folder, you will need the mp4s corresponding to the sample csvs in the data folder. \n",
    "# the mp4 folder should be organized with all of the mp4s in it (do not organize into subfolders for each label/species) \n",
    "# '''\n",
    "\n",
    "# mp4_folder = r\"X:\\Behavior\\DeepLabCut\\Body_Models_2D\\Movement_Curated\\completedata\\rats_treeshrews\\threshold_0.8\\0.8_mp4s_combined\"\n",
    "\n",
    "# output_dir = os.path.join(model_folder, \"output_gifs\")\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ### get all of the unique video names ###\n",
    "# unique_video_names = np.unique([vf[0] for vf in video_frames_sample])\n",
    "\n",
    "# ### get all of the unique dbscan labels ###\n",
    "# unique_labels = np.unique(dbscan_labels)\n",
    "\n",
    "# for video_name in unique_video_names:\n",
    "#     ### get the indices that correspond to the current video name ###\n",
    "#     relevant_indices = [i for i, vf in enumerate(video_frames_sample) if vf[0] == video_name]\n",
    "\n",
    "#     ### extract the video file name from the video_name variable ###\n",
    "#     video_base_name = video_name.split('DLC')[0]\n",
    "#     start_frame = video_name.split('_')[-2]\n",
    "#     end_frame = video_name.split('_')[-1]\n",
    "\n",
    "#     new_video_name = video_base_name + '_' + start_frame + '_' + end_frame\n",
    "\n",
    "#     ### construct the full path to the video file ###\n",
    "#     mp4_path = os.path.join(mp4_folder, new_video_name + '.mp4')\n",
    "\n",
    "#     ### get the mp4 ###\n",
    "#     cap = cv2.VideoCapture(mp4_path)\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"Error opening video file {mp4_path}\")\n",
    "#         continue\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "#     frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "#     ### for each label in all of the dbscan labels ###\n",
    "#     for label in unique_labels:\n",
    "\n",
    "#         ### make a directory for it ###\n",
    "#         label_dir = os.path.join(output_dir, f\"cluster_{label}\")\n",
    "#         os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "#         ### search relevant indeces (the indeces that contain the video) for dbscan labels that match the current label ###\n",
    "#         label_indices = [i for i in relevant_indices if dbscan_labels[i] == label]\n",
    "#         print(label_indices)\n",
    "        \n",
    "#         #### for each indeces that matches both the video and the dbscan label ###\n",
    "#         for i in label_indices:\n",
    "\n",
    "#             vf = video_frames_sample[i]\n",
    "#             frame_index = int(vf[1])\n",
    "\n",
    "#             ### get 0.5 sec before and after ###\n",
    "#             frame_start = max(frame_index - 14, 0)\n",
    "#             frame_end = frame_index + 15\n",
    "\n",
    "#             ### construct the output path for the gif ###\n",
    "#             output_video_path = os.path.join(label_dir, f\"{new_video_name}_frame_{frame_index}.mp4\")\n",
    "#             out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "            \n",
    "#             ### read the first frame ###\n",
    "#             cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start)\n",
    "            \n",
    "#             ### read and write each frame within the sec range ###\n",
    "#             for j in range(frame_start, frame_end + 1):\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret:\n",
    "#                     print(f\"Error reading frame {j} from {new_video_name}\")\n",
    "             \n",
    "#                 out.write(frame)\n",
    "#             ### release the VideoWriter for this segment ###\n",
    "#             out.release()  \n",
    "#     ### release the video capture object after processing the current video ###\n",
    "#     cap.release()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the centroids of each data point on x,y plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(keypoints))\n",
    "keypoints_reshaped = keypoints.reshape(keypoints.shape[0] * keypoints.shape[1], keypoints.shape[2])\n",
    "print(np.shape(keypoints_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### index by the same sample indeces as above ### \n",
    "keypoints_reshaped_sample = keypoints_reshaped[sample_indices]\n",
    "\n",
    "xs = keypoints_reshaped_sample[:,0::2]\n",
    "ys = keypoints_reshaped_sample[:,1::2]\n",
    "\n",
    "print(np.shape(xs))\n",
    "print(np.shape(ys))\n",
    "\n",
    "### average the x and y coordinates for each body part for each data point\n",
    "x_centroids = np.mean(xs, axis = 1)\n",
    "y_centroids = np.mean(ys, axis = 1)\n",
    "\n",
    "print(np.shape(x_centroids))\n",
    "print(np.shape(y_centroids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "### remove the cluster with label -1 ###\n",
    "mask = dbscan_labels != -1\n",
    "filtered_x_centroids = x_centroids[mask]\n",
    "filtered_y_centroids = y_centroids[mask]\n",
    "filtered_labels = dbscan_labels[mask]\n",
    "\n",
    "### get all of the unique dbscan labels ###\n",
    "unique_labels = np.unique(filtered_labels)\n",
    "\n",
    "### make a color map for the unique labels ###\n",
    "colors = plt.cm.get_cmap('tab20', len(unique_labels))  \n",
    "\n",
    "### map each unique label to a color ###\n",
    "color_mapping = {label: colors(i) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "### map the filtered labels to the corresponding colors ###\n",
    "color_list = [color_mapping[label] for label in filtered_labels]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(filtered_x_centroids, filtered_y_centroids, c=color_list, s=50, alpha=0.7)\n",
    "\n",
    "# ### add the cluter labels next to each dot ###\n",
    "# for i, label in enumerate(filtered_labels):\n",
    "#      plt.text(filtered_x_centroids[i], filtered_y_centroids[i], str(label), fontsize=9, ha='right')\n",
    "\n",
    "### adding plot title and labels ###\n",
    "plt.xlabel(\"X Coordinate\")\n",
    "plt.ylabel(\"Y Coordinate\")\n",
    "\n",
    "#### create a custom legend ###\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_mapping[label], markersize=10) for label in unique_labels]\n",
    "plt.legend(handles, unique_labels, title=\"Cluster Label\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### uncomment if you want to save the centroids plot ###\n",
    "#plt.savefig(os.path.join(model_folder,\"centroids.png\"),bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "### remove the cluster with label -1 ###\n",
    "mask = dbscan_labels != -1\n",
    "filtered_x_centroids = x_centroids[mask]\n",
    "filtered_y_centroids = y_centroids[mask]\n",
    "filtered_labels = dbscan_labels[mask]\n",
    "\n",
    "### get all of the unique dbscan labels ###\n",
    "unique_labels = np.unique(filtered_labels)\n",
    "\n",
    "\n",
    "### make a color map for the unique labels ###\n",
    "colors = [f'rgba({int(np.random.rand()*255)}, {int(np.random.rand()*255)}, {int(np.random.rand()*255)}, 0.8)' for _ in unique_labels]\n",
    "color_mapping = {label: color for label, color in zip(unique_labels, colors)}\n",
    "\n",
    "# Assign colors to each point based on its label\n",
    "color_list = [color_mapping[label] for label in filtered_labels]\n",
    "\n",
    "### create the scatter plot ###\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=filtered_x_centroids,\n",
    "    y=filtered_y_centroids,\n",
    "    mode='markers+text',\n",
    "    text=filtered_labels,\n",
    "    textposition='top right',\n",
    "    marker=dict(color=color_list, size=10, opacity=0.7),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "### customize axes and layout ###\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"X Coordinate\",\n",
    "    yaxis_title=\"Y Coordinate\",\n",
    "    title=\"DBSCAN Cluster Centroids\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    scaleanchor=\"x\",\n",
    "    scaleratio=1,\n",
    "  )\n",
    "\n",
    "fig.show()\n",
    "\n",
    "### uncomment if you want to save the centroids plot ###\n",
    "#fig.write_html(os.path.join(model_folder,\"centroids_plotly.html\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bams_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
